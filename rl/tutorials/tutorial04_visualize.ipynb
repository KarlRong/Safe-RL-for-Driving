{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 04: Visualizing Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial describes the process of visualizing the results of Flow experiments, and of replaying them. \n",
    "\n",
    "**Note:** This tutorial is only relevant if you use SUMO as a simulator. We currently do not support policy replay nor data collection when using Aimsun. The only exception is for reward plotting, which is independent on whether you have used SUMO or Aimsun during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualization components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization of simulation results breaks down into three main components:\n",
    "\n",
    "- **reward plotting**: Visualization of the reward function is an essential step in evaluating the effectiveness and training progress of RL agents.\n",
    "\n",
    "- **policy replay**: Flow includes tools for visualizing trained policies using SUMO's GUI. This enables more granular analysis of policies beyond their accrued reward, which in turn allows users to tweak actions, observations and rewards in order to produce some desired behavior. The visualizers also generate plots of observations and a plot of the reward function over the course of the rollout.\n",
    "\n",
    "- **data collection and analysis**: Any Flow experiment can output its simulation data to a CSV file, `emission.csv`, containing the contents of SUMO's built-in `emission.xml` files. This file contains various data such as the speed, position, time, fuel consumption and many other metrics for every vehicle in the network and at each time step of the simulation. Once you have generated the `emission.csv` file, you can open it and read the data it contains using Python's [csv library](https://docs.python.org/3/library/csv.html) (or using Excel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization is different depending on which reinforcement learning library you are using, if any. Accordingly, the rest of this tutorial explains how to plot rewards, replay policies and collect data when using either no RL library, RLlib, or stable-baselines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "\n",
    "[How to visualize using SUMO without training](#2.1---Using-SUMO-without-training)\n",
    "\n",
    "[How to visualize using SUMO with RLlib](#2.2---Using-SUMO-with-RLlib)\n",
    "\n",
    "[**_Example: visualize data on a ring trained using RLlib_**](#2.3---Example:-Visualize-data-on-a-ring-trained-using-RLlib)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Using SUMO without training\n",
    "\n",
    "_In this case, since there is no training, there is no reward to plot and no policy to replay._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection and analysis\n",
    "\n",
    "SUMO-only experiments can generate emission CSV files seamlessly:\n",
    "\n",
    "First, you have to tell SUMO to generate the `emission.xml` files. You can do that by specifying `emission_path` in the simulation parameters (class `SumoParams`), which is the path where the emission files will be generated. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sim_params = SumoParams(sim_step=0.1, render=True, emission_path='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you have to tell Flow to convert these XML emission files into CSV files. To do that, pass in `convert_to_csv=True` to the `run` method of your experiment object. For instance:\n",
    "\n",
    "```python\n",
    "exp.run(1, convert_to_csv=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running experiments, Flow will now automatically create CSV files next to the SUMO-generated XML files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Using SUMO with RLlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward plotting\n",
    "\n",
    "RLlib supports reward visualization over the period of the training using the `tensorboard` command. It takes one command-line parameter, `--logdir`, which is an RLlib result directory. By default, it would be located within an experiment directory inside your `~/ray_results` directory. \n",
    "\n",
    "An example call would look like:\n",
    "\n",
    "`tensorboard --logdir ~/ray_results/experiment_dir/result/directory`\n",
    "\n",
    "You can also run `tensorboard --logdir ~/ray_results` if you want to select more than just one experiment.\n",
    "\n",
    "If you do not wish to use `tensorboard`, an other way is to use our `flow/visualize/plot_ray_results.py` tool. It takes as arguments:\n",
    "\n",
    "- the path to the `progress.csv` file located inside your experiment results directory (`~/ray_results/...`),\n",
    "- the name(s) of the column(s) you wish to plot (reward or other things).\n",
    "\n",
    "An example call would look like:\n",
    "\n",
    "`flow/visualize/plot_ray_results.py ~/ray_results/experiment_dir/result/progress.csv training/return-average training/return-min`\n",
    "\n",
    "If you do not know what the names of the columns are, run the command without specifying any column:\n",
    "\n",
    "`flow/visualize/plot_ray_results.py ~/ray_results/experiment_dir/result/progress.csv`\n",
    "\n",
    "and the list of all available columns will be displayed to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy replay\n",
    "\n",
    "The tool to replay a policy trained using RLlib is located at `flow/visualize/visualizer_rllib.py`. It takes as argument, first the path to the experiment results (by default located within `~/ray_results`), and secondly the number of the checkpoint you wish to visualize (which correspond to the folder `checkpoint_<number>` inside the experiment results directory).\n",
    "\n",
    "An example call would look like this:\n",
    "\n",
    "`python flow/visualize/visualizer_rllib.py ~/ray_results/experiment_dir/result/directory 1`\n",
    "\n",
    "There are other optional parameters which you can learn about by running `visualizer_rllib.py --help`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection and analysis\n",
    "\n",
    "Simulation data can be generated the same way as it is done [without training](#2.1---Using-SUMO-without-training).\n",
    "\n",
    "If you need to generate simulation data after the training, you can run a policy replay as mentioned above, and add the `--gen-emission` parameter.\n",
    "\n",
    "An example call would look like:\n",
    "\n",
    "`python flow/visualize/visualizer_rllib.py ~/ray_results/experiment_dir/result/directory 1 --gen_emission`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Example: Visualize data on a ring trained using RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rong/flow/tutorials\n"
     ]
    }
   ],
   "source": [
    "!pwd  # make sure you are in the flow/tutorials folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder `flow/tutorials/data/trained_ring` contains the data generated in `ray_results` after training an agent on a ring scenario for 200 iterations using RLlib (the experiment can be found in `flow/examples/rllib/stabilizing_the_ring.py`).\n",
    "\n",
    "Let's first have a look at what's available in the `progress.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns are: episode_reward_max, episode_reward_min, episode_reward_mean, episode_len_mean, episodes_this_iter, timesteps_this_iter, done, timesteps_total, episodes_total, training_iteration, experiment_id, date, timestamp, time_this_iter_s, time_total_s, pid, hostname, node_ip, time_since_restore, timesteps_since_restore, iterations_since_restore, num_healthy_workers, trial_id, sampler_perf/mean_env_wait_ms, sampler_perf/mean_processing_ms, sampler_perf/mean_inference_ms, info/num_steps_trained, info/num_steps_sampled, info/sample_time_ms, info/load_time_ms, info/grad_time_ms, info/update_time_ms, perf/cpu_util_percent, perf/ram_util_percent, info/learner/default_policy/cur_kl_coeff, info/learner/default_policy/cur_lr, info/learner/default_policy/total_loss, info/learner/default_policy/policy_loss, info/learner/default_policy/vf_loss, info/learner/default_policy/vf_explained_var, info/learner/default_policy/kl, info/learner/default_policy/entropy, info/learner/default_policy/entropy_coeff\n"
     ]
    }
   ],
   "source": [
    "!python ../flow/visualize/plot_ray_results.py data/trained_ring/progress.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a list of everything that we can plot. Let's plot the reward and its boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'./flow/visualize/plot_ray_results.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "# if this doesn't display anything, try with \"%matplotlib inline\" instead\n",
    "%run ../flow/visualize/plot_ray_results.py data/trained_ring/progress.csv \\\n",
    "episode_reward_mean episode_reward_min episode_reward_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the policy had already converged by the iteration 50.\n",
    "\n",
    "Now let's see what this policy looks like. Run the following script, then click on the green arrow to run the simulation (you may have to click several times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rong/.conda/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rong/.conda/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rong/.conda/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rong/.conda/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rong/.conda/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rong/.conda/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2020-02-07 00:44:00,129\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-02-07_00-44-00_129070_13017/logs.\n",
      "2020-02-07 00:44:00,233\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:56607 to respond...\n",
      "2020-02-07 00:44:00,340\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:23788 to respond...\n",
      "2020-02-07 00:44:00,341\tINFO services.py:809 -- Starting Redis shard with 6.74 GB max memory.\n",
      "2020-02-07 00:44:00,354\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-02-07_00-44-00_129070_13017/logs.\n",
      "2020-02-07 00:44:00,355\tINFO services.py:1475 -- Starting the Plasma object store with 10.11 GB memory using /dev/shm.\n",
      "2020-02-07 00:44:00,401\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n",
      "2020-02-07 00:44:00,408\tWARNING ppo.py:143 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "2020-02-07 00:44:01,566\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "2020-02-07 00:44:01.567627: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2020-02-07 00:44:01,656\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\n",
      "{ 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "  'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 1) dtype=float32>,\n",
      "  'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "  'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "  'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "  'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 3) dtype=float32>,\n",
      "  'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 3) dtype=float32>,\n",
      "  'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 1) dtype=float32>,\n",
      "  'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "  'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "  'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "  'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\n",
      "/home/rong/.conda/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2020-02-07 00:44:02,156\tINFO rollout_worker.py:742 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f54182422b0>}\n",
      "2020-02-07 00:44:02,156\tINFO rollout_worker.py:743 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f54184edf28>}\n",
      "2020-02-07 00:44:02,156\tINFO rollout_worker.py:356 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f54184eddd8>}\n",
      "2020-02-07 00:44:02,159\tINFO multi_gpu_optimizer.py:93 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "2020-02-07 00:44:03,657\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n",
      "\n",
      "-----------------------\n",
      "ring length: 249\n",
      "v_max: 4.617190445131122\n",
      "-----------------------\n",
      "2020-02-07 00:44:12,754\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "Round 0, Return: 1648.5170771862408\n",
      "==== Summary of results ====\n",
      "Return:\n",
      "[4.179592571532758]\n",
      "[1648.5170771862408]\n",
      "Average, std: 1648.5170771862408, 0.0\n",
      "\n",
      "Speed, mean (m/s):\n",
      "[4.179592571532758]\n",
      "Average, std: 4.179592571532758, 0.0\n",
      "\n",
      "Speed, std (m/s):\n",
      "[0.11054596661323703]\n",
      "Average, std: 0.11054596661323703, 0.0\n",
      "\n",
      "Outflows (veh/hr):\n",
      "[0.0]\n",
      "Average, std: 0.0, 0.0\n",
      "Inflows (veh/hr):\n",
      "[0.0]\n",
      "Average, std: 0.0, 0.0\n",
      "Throughput efficiency (veh/hr):\n",
      "[0]\n",
      "Average, std: 0.0, 0.0\n"
     ]
    }
   ],
   "source": [
    "!python ../flow/visualize/visualizer_rllib.py data/trained_ring 200 --horizon 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL agent is properly stabilizing the ring! \n",
    "\n",
    "Indeed, without an RL agent, the vehicles start forming stop-and-go waves which significantly slows down the traffic, as you can see in this simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0, return: 424.12213238365126\n",
      "Average, std return: 424.12213238365126, 0.0\n",
      "Average, std speed: 2.883939027587335, 0.0\n"
     ]
    }
   ],
   "source": [
    "!python ../examples/simulate.py ring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the trained ring folder, there is a checkpoint generated every 20 iterations. Try to run the second previous command but replace 200 by 20. On the reward plot, you can see that the reward is already quite high at iteration 20, but hasn't converged yet, so the agent will perform a little less well than at iteration 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this example! Feel free to play around with the other scripts in `flow/visualize`. Run them with the `--help` parameter and it should tell you how to use it. Also, if you need the emission file for the trained ring, you can obtain it by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python ../flow/visualize/visualizer_rllib.py data/trained_ring 200 --horizon 2000 --gen_emission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path where the emission file is generated will be outputted at the end of the simulation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
